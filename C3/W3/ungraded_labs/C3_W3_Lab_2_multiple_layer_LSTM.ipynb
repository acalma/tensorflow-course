{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFiCyWQ-NC5D"
   },
   "source": [
    "# Ungraded Lab: Multiple LSTMs\n",
    "\n",
    "In this lab, you will look at how to build a model with multiple LSTM layers. Since you know the preceding steps already (e.g. downloading datasets, preparing the data, etc.), we won't expound on it anymore so you can just focus on the model building code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqmDNHeByJqr"
   },
   "source": [
    "## Download and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T14:53:27.736518Z",
     "start_time": "2024-05-19T14:53:24.383008Z"
    },
    "id": "AW-4Vo4TMUHb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n",
      "2024-05-19 14:53:25.209282: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-19 14:53:25.257941: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-19 14:53:25.257971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-19 14:53:25.259090: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-19 14:53:25.266628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 14:53:26.160841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-19 14:53:27.168599: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.214472: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.216564: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.219211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.221222: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.223195: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.359541: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.360738: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.361784: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 14:53:27.362772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Download the subword encoded pretokenized dataset\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T14:53:27.742077Z",
     "start_time": "2024-05-19T14:53:27.738694Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF8bUh_5Ff7y"
   },
   "source": [
    "Like the previous lab, we increased the `BATCH_SIZE` here to make the training faster. If you are doing this on your local machine and have a powerful processor, feel free to use the value used in the lecture (i.e. 64) to get the same results as Laurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T14:53:27.764320Z",
     "start_time": "2024-05-19T14:53:27.743514Z"
    },
    "id": "ffvRUI0_McDS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Get the train and test splits\n",
    "train_data, test_data = dataset['train'], dataset['test'], \n",
    "\n",
    "# Shuffle the training data\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Batch and pad the datasets to the maximum length of the sequences\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcZEiG9ayNZr"
   },
   "source": [
    "## Build and Compile the Model\n",
    "\n",
    "You can build multiple layer LSTM models by simply appending another `LSTM` layer in your `Sequential` model and enabling the `return_sequences` flag to `True`. This is because an `LSTM` layer expects a sequence input so if the previous layer is also an LSTM, then it should output a sequence as well. See the code cell below that demonstrates this flag in action. You'll notice that the output dimension is in 3 dimensions `(batch_size, timesteps, features)` when when `return_sequences` is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T14:53:28.069200Z",
     "start_time": "2024-05-19T14:53:27.766022Z"
    },
    "id": "18MsI2LU75kH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "timesteps (sequence length): 20\n",
      "features (embedding size): 16\n",
      "lstm output units: 8\n",
      "shape of input array: (1, 20, 16)\n",
      "shape of lstm output(return_sequences=False): (1, 8)\n",
      "shape of lstm output(return_sequences=True): (1, 20, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 14:53:28.026535: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "timesteps = 20\n",
    "features = 16\n",
    "lstm_dim = 8\n",
    "\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'timesteps (sequence length): {timesteps}')\n",
    "print(f'features (embedding size): {features}')\n",
    "print(f'lstm output units: {lstm_dim}')\n",
    "\n",
    "# Define array input with random values\n",
    "random_input = np.random.rand(batch_size,timesteps,features)\n",
    "print(f'shape of input array: {random_input.shape}')\n",
    "\n",
    "# Define LSTM that returns a single output\n",
    "lstm = tf.keras.layers.LSTM(lstm_dim)\n",
    "result = lstm(random_input)\n",
    "print(f'shape of lstm output(return_sequences=False): {result.shape}')\n",
    "\n",
    "# Define LSTM that returns a sequence\n",
    "lstm_rs = tf.keras.layers.LSTM(lstm_dim, return_sequences=True)\n",
    "result = lstm_rs(random_input)\n",
    "print(f'shape of lstm output(return_sequences=True): {result.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Was3BX6_50C"
   },
   "source": [
    "The next cell implements the stacked LSTM architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T14:53:29.042538Z",
     "start_time": "2024-05-19T14:53:28.070792Z"
    },
    "id": "VPNwU1SVyTjm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          523840    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 128)         66048     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 635329 (2.42 MB)\n",
      "Trainable params: 635329 (2.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "lstm1_dim = 64\n",
    "lstm2_dim = 32\n",
    "dense_dim = 64\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),\n",
    "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T14:53:29.060868Z",
     "start_time": "2024-05-19T14:53:29.047224Z"
    },
    "id": "Uip7QOVzMoMq"
   },
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh39GlZP79DY"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "The additional LSTM layer will lengthen the training time compared to the previous lab. Given the default parameters we set, it will take around 2 minutes per epoch with the Colab GPU enabled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:00:16.602442Z",
     "start_time": "2024-05-19T14:53:45.075872Z"
    },
    "id": "7mlgzaRDMtF6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 - 47s - loss: 0.5304 - accuracy: 0.7333 - val_loss: 0.4300 - val_accuracy: 0.8106 - 47s/epoch - 478ms/step\n",
      "Epoch 2/10\n",
      "98/98 - 41s - loss: 0.3089 - accuracy: 0.8788 - val_loss: 0.3967 - val_accuracy: 0.8430 - 41s/epoch - 422ms/step\n",
      "Epoch 3/10\n",
      "98/98 - 41s - loss: 0.2229 - accuracy: 0.9170 - val_loss: 0.4035 - val_accuracy: 0.8411 - 41s/epoch - 415ms/step\n",
      "Epoch 4/10\n",
      "98/98 - 39s - loss: 0.1725 - accuracy: 0.9384 - val_loss: 0.4349 - val_accuracy: 0.8498 - 39s/epoch - 395ms/step\n",
      "Epoch 5/10\n",
      "98/98 - 39s - loss: 0.1262 - accuracy: 0.9571 - val_loss: 0.5308 - val_accuracy: 0.8175 - 39s/epoch - 396ms/step\n",
      "Epoch 6/10\n",
      "98/98 - 38s - loss: 0.0928 - accuracy: 0.9709 - val_loss: 0.5485 - val_accuracy: 0.8351 - 38s/epoch - 386ms/step\n",
      "Epoch 7/10\n",
      "98/98 - 37s - loss: 0.0672 - accuracy: 0.9810 - val_loss: 0.6591 - val_accuracy: 0.8288 - 37s/epoch - 378ms/step\n",
      "Epoch 8/10\n",
      "98/98 - 37s - loss: 0.0802 - accuracy: 0.9745 - val_loss: 0.6326 - val_accuracy: 0.8195 - 37s/epoch - 373ms/step\n",
      "Epoch 9/10\n",
      "98/98 - 37s - loss: 0.1052 - accuracy: 0.9642 - val_loss: 0.6593 - val_accuracy: 0.8038 - 37s/epoch - 375ms/step\n",
      "Epoch 10/10\n",
      "98/98 - 37s - loss: 0.0719 - accuracy: 0.9780 - val_loss: 0.7203 - val_accuracy: 0.8220 - 37s/epoch - 374ms/step\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T14:53:41.407518Z",
     "start_time": "2024-05-19T14:53:41.407061Z"
    },
    "id": "Mp1Z7P9pYRSK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "# Plot the accuracy and results \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txQdN63vBlTK"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "This lab showed how you can build deep networks by stacking LSTM layers. In the next labs, you will continue exploring other architectures you can use to implement your sentiment classification model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W3_Lab_2_multiple_layer_LSTM.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
